os:
  - linux
dist: xenial
language: scala
jdk:
  - openjdk8
python:
  - "3.7"
branches:
  only:
  - spark-2.3-2.4
  - master
jobs:
  include:
  - scala: 2.11.12
    env: SPARK_VERSION=2.4.6
    if: branch = spark-2.3-2.4
  - scala: 2.12.12
    env: SPARK_VERSION=3.0.0
    if: branch = spark-3.0
before_install:
  - sudo apt-get -y install python3-pip python-dev
  - sudo pip3 install -U setuptools
  - sudo pip3 install -U wheel
  - sudo pip3 install -U virtualenvwrapper
install:
- python3 -m pip install pipenv
- wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
- tar -xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
- rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz

script:
  - (cd python;pipenv install --dev) 
  - export SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-hadoop2.7
  - export PYTHONPATH=$SPARK_HOME/python
  - mvn -q clean install -Dscala.version=${TRAVIS_SCALA_VERSION}  -Dscala.compat.version=${TRAVIS_SCALA_VERSION%.*} -Dspark.version=${SPARK_VERSION} -Dspark.compat.version=${SPARK_VERSION%.*}
  - find core/target/ -iregex "core\/target\/geospark-[0-9]\.[0-9]\.[0-9]\.jar" -exec cp {} $SPARK_HOME/jars \;
  - find sql/target/ -iregex "sql\/target\/geospark-sql_[0-9]\.[0-9]-[0-9]\.[0-9]\.[0-9]\.jar" -exec cp {} $SPARK_HOME/jars \;
  - (cd python;pipenv run pytest tests)

